# -*- coding: utf-8 -*-
"""chatbotv3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1csWW_S0ugSpz7G2vsneRtLGumC-eTEXr
"""

!pip install --qu langgraph langchain_core langchain-google-genai langchain_community
#!pip install --qu langgraph-sdk
!pip install --qu  tavily-python

from google.colab import userdata
#from dotenv import load_dotenv
#load_dotenv('.env')
import random
from langchain_core.messages import SystemMessage,AIMessage,HumanMessage,BaseMessage,AnyMessage

gemini_api_key=userdata.get("GOOGLE_API_KEY")
tavily_api_key=userdata.get("TAVILY_API_KEY")

from langchain_google_genai import ChatGoogleGenerativeAI
llm=ChatGoogleGenerativeAI(model="gemini-1.5-flash", api_key=gemini_api_key)
#result=llm.invoke(messages)
#type(result)

from langchain_community.tools.tavily_search import TavilySearchResults

"""Tool"""

def multiply(a:int,b:int)->int:
  """Multiply a and b
  Args:
    a:int
    b:int
  Returns:
    int:a*b
  """
  return a*b

def add (a:int,b:int)->int:
    """Add a and b
    Args:
    a:int
    b:int
    Returns:
    int:a*b
    """
    return a+b

from tavily import TavilyClient

def find(a: str)->str:
  """ Return search results from TavilySearch
  """
  tavily_client = TavilyClient(api_key=tavily_api_key)
  response = tavily_client.search(a)

  return {"response": response}

tools=[multiply,add,find]

llm=ChatGoogleGenerativeAI(model='gemini-1.5-flash', api_key=gemini_api_key)

llm_with_tools=llm.bind_tools(tools)

from langgraph.graph import MessagesState
from langchain_core.messages import SystemMessage,AIMessage,HumanMessage,BaseMessage,AnyMessage
sys_message=[SystemMessage(content="You are a helpful assistant tasked with performing arithmatic on set of inputs")]

def assistant1(state:MessagesState):
  return{ "messages":[llm.invoke(state["messages"][-1:])]}

    #{"messages":[llm_with_tools.invoke([sys_message]+state["messages"])]}

def assistant(state:MessagesState):
  return{"messages":[llm_with_tools.invoke(sys_message + state["messages"])]}
  # Remove extra brackets around sys_message

from langgraph.graph import START,END, StateGraph
from langgraph.prebuilt import tools_condition
from langgraph.prebuilt import ToolNode
from IPython.display import Image, display
from langgraph.checkpoint.memory import MemorySaver



builder=StateGraph(MessagesState)
builder.add_node("assistant", assistant)
builder.add_node("tools",ToolNode(tools))
builder.add_edge(START,"assistant")
builder.add_conditional_edges("assistant",tools_condition)
builder.add_edge("tools", "assistant")
memory =MemorySaver()
#graph=builder.comppile()
react_graph=builder.compile(interrupt_before=['tools'],checkpointer=memory)

display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))

config={
    "configurable":{"thread_id": "1"}
}

messages=[HumanMessage(content="find about Summer Olympic games where these were held last time")]
messages=react_graph.invoke({"messages":messages},config)

for m in messages['messages']:
  m.pretty_print()

state=react_graph.get_state(config)
state.next

for event in react_graph.stream(None,config, stream_mode="values"):
  event ['messages'][-1].pretty_print()

#messages=[HumanMessage(content=" add 2 in that and then divide from 4 also tell me about AI")]
#messages=react_graph.invoke({"messages":messages},config)

#for m in messages['messages']:
  #m.pretty_print()

initial_input= " who won gold medal in volley ball"
config2={
    "configurable":{"thread_id": "2"}
}

for  event in react_graph.stream(None,config, stream_mode="values"):
  event ['messages'][-1].pretty_print()
user_approval = input("do you want to call tool? (y/n)")
if user_approval == "y":
  for  event in react_graph.stream(None,config, stream_mode="values"):
    event ['messages'][-1].pretty_print()
else:
  print("operation canceled by user")